---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am now an associate researcher at the School of Software, Shandong University. Before my current position, I was a postdoc working with <a href="https://www.cs.sfu.ca/~haoz/">Prof. Hao (Richard) Zhang</a> in <a href="https://gruvi.cs.sfu.ca/">GrUVi Lab</a> at Simon Fraser University, Canada. I was also a intern research scientist in <a href="https://ips-ai.com/">Intelligent Project Solutions Inc.</a> working on Layout digitalization and editing projects. I got my Ph.D degree on December 2018 from Shandong University, supervised by <a href="http://irc.cs.sdu.edu.cn/~chtu/">Prof. Changhe Tu</a>. During my Ph.D career, I have visited and collaborated closely with 
                <a href="https://www.cs.sfu.ca/~haoz/">Prof. Hao (Richard) Zhang</a> (supported by 
                <a href="https://www.chinesescholarshipcouncil.com/">China Scholarship Council (CSC)</a>),
                <a href="http://www.math.tau.ac.il/~dcor/">Prof. Daniel Cohen-Or</a>,
                <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Prof. Wenping Wang</a>,
                <a href="http://staff.ustc.edu.cn/~chenfl/english.htm">Prof. Falai Chen</a>.

My research field lies in Computer Graphics and Computer Vision. More specifically, my research interest focuses on the understanding, generation, and interaction of 3D indoor scenes, including object reconstruction and segmentation, 3D shape analysis and generation, scene layout synthesis and editing, hand-object interaction generation, etc.

李曼祎，山东大学软件学院人机交互与虚拟现实中心副研究员，硕士生导师。2018年12月获得山东大学工学博士学位，导师为屠长河教授。读博期间曾在香港大学、中国科学技术大学、特拉维夫大学、西蒙弗雷泽大学交流访问。2019-2021年在加拿大西蒙弗雷泽大学GrUVi实验室从事博士后研究，导师为Hao (Richard) Zhang教授。

研究领域为计算机图形学、三维视觉、人工智能等，主要关注三维室内场景的理解、生成与交互，包括三维物体重建与分割、形状分析与生成、场景布局生成与编辑、手-物交互生成等。曾在ACM Transactions on Graphics (TOG)、Siggraph、CVPR、ICCV、TVCG等顶级国际会议和期刊上发表论文十余篇。担任中国计算机学会计算机辅助设计与图形学专委会委员、中国工业与应用数学学会几何设计与计算专委会秘书处委员等。 目前主持国家自然科学基金青年项目、山东省优秀青年科学基金项目（海外）、山东大学未来计划等项目。获得2024年CCF科技成果自然科学一等奖、2022年ACM济南分会新星奖。


# 🔥 News
- *2022.02*: &nbsp;🎉🎉 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;🎉🎉 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# 📝 Publications 

<!-- CGF 2025  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CGF 2025</div><img src='images/paper_imgs/2025_diffusion_model_for_point_clouds.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model**

Zhaonan Wang , **Manyi Li\***, Shiqing Xin , Changhe Tu

**Paper** \| **Project**

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVMJ 2025  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVMJ 2025</div><img src='images/paper_imgs/2025_object_aware_transfer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Object-Aware Appearance Transfer for Interior Design**

Ruisi Ye, **Manyi Li\***, Xifeng Gao, Changhe Tu

**Paper** \| **Project**

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVPR 2025  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/paper_imgs/2025_freescene.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts**

Tongyuan Bai， Wangyuanfan Bai， Dong Chen， Tieru Wu, **Manyi Li**， Rui Ma

**Paper** \| **Project**

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- AAAI 2025  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images/paper_imgs/2025_hierarchical_scenes.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Hierarchically-Structured Open-Vocabulary Indoor Scene Synthesis with Pre-trained Large Language Model**

Weilin Sun, Xinran Li, **Manyi Li**, 
<a href="https://kevinkaixu.net/">Kai Xu</a>, Xiangxu Meng, 
<a href="https://ercdm.sdu.edu.cn/info/1013/1523.htm">Lei Meng</a>

[**Paper**](https://arxiv.org/abs/2502.10675) \| [**Project**](https://github.com/SunWeiLin-Lynne/Hierarchically-Structured-Open-Vocabulary-Indoor-Scene-Synthesis/tree/main)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- Siggraph 2024  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Siggraph 2024</div><img src='images/paper_imgs/2024_dreamfont.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**DreamFont3D: Personalized Text-to-3D Artistic Font Generation**

Xiang Li, 
<a href="https://ercdm.sdu.edu.cn/info/1013/1523.htm">Lei Meng</a>, 
Lei Wu, **Manyi Li**, Xiangxu Meng

[**Paper**] \| [**Project**](https://moonlight03.github.io/DreamFont3D/)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CGF 2024  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CGF 2024</div><img src='images/paper_imgs/2024_scene_survey.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Advances in Data-Driven Analysis and Synthesis of 3D Indoor Scenes**

<a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>,
		      Supriya Gadi Patil,
		      **Manyi Li\***,
		      <a href="https://techmatt.github.io/">Matthew Fisher</a>,
		      <a href="http://msavva.github.io/">Manolis Savva</a>,
		      <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,

[**Paper**](https://arxiv.org/pdf/2304.03188.pdf) 

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- Siggraph Asia 2023  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Siggraph Asia 2024</div><img src='images/paper_imgs/2023_uvpack.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Learning Based 2D Irregular Shape Packing**

<a href="https://zeshiyang.github.io/">Zeshi Yang</a>,
		      <a>Zherong Pan</a>,
		      **Manyi Li**,
		      <a href="https://kuiwuchn.github.io/">Kui Wu</a>,
		      <a href="https://gaoxifeng.github.io/">Xifeng Gao</a>,

[**Paper**](https://arxiv.org/pdf/2309.10329.pdf) 

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- ICCV 2023  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/paper_imgs/2023_afford_pose.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**AffordPose: A Large-scale Dataset of Hand-Object Interactions with Affordance-driven Hand Pose**

Juntao Jian, Xiuping Liu, **Manyi Li\***, 
<a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, 
<a href="https://jianliu2006.github.io/ ">Jian Liu*</a>

[**Paper**](https://arxiv.org/pdf/2309.08942.pdf) \| [**Project**](https://github.com/GentlesJan/AffordPose)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVMJ 2023  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVMJ 2023</div><img src='images/paper_imgs/2023_dancepro.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Let's all dance: Enhancing amateur dance motions**

Qiu Zhou#, **Manyi Li\#**, 
<a href="https://qiongzn.github.io/">Qiong Zeng</a>,
		      <a href="http://www.andreasaristidou.com/">Andreas Aristidou</a>, Xiaojing Zhang, Lin Chen, 
          <a href="http://irc.cs.sdu.edu.cn/~chtu/index.html">Changhe Tu</a>

[**Paper**](http://irc.cs.sdu.edu.cn/dancepro/dance_cvm.pdf) \| [**Project**](http://irc.cs.sdu.edu.cn/dancepro/index.html)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- TVCG 2023  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TVCG 2023</div><img src='images/paper_imgs/2023_laplacian2mesh.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Laplacian2mesh: Laplacian-based mesh understanding**

Qiujie Dong, 
<a href="https://bearprin.com/">Zixiong Wang</a>, 
**Manyi Li**, 
Junjie Gao, Shuang-Min Chen, Zhenyu Shu, 
<a href="https://irc.cs.sdu.edu.cn/~shiqing/index.html">Shiqing Xin</a>,
		      <a href="http://irc.cs.sdu.edu.cn/~chtu/index.html">Changhe Tu</a>,
		      <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a>

[**Paper**](https://arxiv.org/pdf/2202.00307.pdf) \| [**Project**](https://github.com/QiujieDong/Laplacian2Mesh)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVPR 2022  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/paper_imgs/2022_RIMNet_long.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Rim-net: Recursive implicit fields for unsupervised learning of hierarchical shape structures**

<a href="https://chengjieniu.github.io/">Chengjie Niu</a>,
              **Manyi Li**,
              <a href="https://kevinkaixu.net/">Kai Xu</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,

[**Paper**](https://arxiv.org/abs/2201.12763) \| [**Project**](https://github.com/chengjieniu/RIM-Net)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVPR 2022  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/paper_imgs/2022_capri_net.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly**

<a href="https://fenggenyu.github.io/">Fenggen Yu</a>,
              <a href="https://czq142857.github.io/">Zhiqin Chen</a>,
              **Manyi Li**,
			  <a>Aditya Sanghi</a>,
			  <a>Hooman Shayani</a>,
              <a href="https://sites.google.com/site/alimahdaviamiri/home">Ali Mahdavi-Amiri</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>

[**Paper**](https://arxiv.org/abs/2104.05652) \| [**Project**](https://fenggenyu.github.io/capri.html)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVPR 2021  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2021</div><img src='images/paper_imgs/2021_d2im_image.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**D^2IM-Net: Learning Detail Disentangled Implicit Fields from Single Images**

**Manyi Li** and <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>

[**Paper**](https://arxiv.org/abs/2012.06650) \| [**Project**](https://github.com/ManyiLi12345/D2IM-Net)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVPR 2021  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2021</div><img src='images/paper_imgs/2021_LayoutGMN_image.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**LayoutGMN: Neural Graph Matching for Structural Layout Similarity**

<a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>, 
**Manyi Li**,
              <a href="https://research.adobe.com/person/matt-fisher/">Matthew Fisher</a>,
              <a href="http://msavva.github.io/">Manolis Savva</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>

[**Paper**](https://arxiv.org/abs/2012.06547) \| [**Project**](https://github.com/akshaygadipatil/LayoutGMN-pytorch)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- TOG 2019  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TOG 2019</div><img src='images/paper_imgs/2019_grains_image.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**GRAINS: Generative Recursive Autoencoders for INdoor Scenes**

**Manyi Li**,
              <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>,
              <a href="https://kevinkaixu.net/">Kai Xu</a>,
              <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>,
              Owais Khan,
              <a href="https://faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>,
              <a href="http://irc.cs.sdu.edu.cn/~chtu/">Changhe Tu</a>,
              <a href="http://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>,
              <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,

[**Paper**](https://arxiv.org/abs/1807.09193) \| [**Project**](./Publication/2018/GRAINS/index.html)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- Siggraph Asia 2018  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Siggraph Asia 2018</div><img src='images/paper_imgs/2018_T2S_image.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Language-driven synthesis of 3D scenes from scene databases**

<a href="https://ruim-jlu.github.io/">Rui Ma#</a>,
              <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil#</a>,
              <a href="https://techmatt.github.io/">Matthew Fisher</a>,
              **Manyi Li**,
              <a href="https://storage.googleapis.com/pirk.io/index.html">Sören Pirk</a>,
              <a href="http://sonhua.github.io/">Binh-Son Hua</a>,
              <a href="http://www.saikit.org/">Sai-Kit Yeung</a>,
              <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>,
              <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,

[**Paper**](./Publication/2018/T2S/t2s_final.pdf) \| [**Project**](./Publication/2018/T2S/index.html)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- GMOD 2018  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GMOD 2018</div><img src='images/paper_imgs/2018_ShapeDist_image.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Class-sensitive shape dissimilarity metric**

**Manyi Li**, <a href="https://www.cs.tau.ac.il/~noafish/">Noa Fish</a>,
              Lili Cheng,
              <a href="http://irc.cs.sdu.edu.cn/~chtu/">Changhe Tu</a>,
              <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,  
              <a href="http://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>

[**Paper**](https://www.sciencedirect.com/science/article/pii/S1524070318300328?via%3Dihub)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CAGD 2016  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GMOD 2018</div><img src='images/paper_imgs/2016_SparseRBF_image.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Sparse RBF surface representations**

**Manyi Li**, <a href="http://staff.ustc.edu.cn/~chenfl/english.htm">Falai Chen</a>,
              <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Wenping Wang</a>,
              <a href="http://irc.cs.sdu.edu.cn/~chtu/">Changhe Tu</a>

[**Paper**](https://www.sciencedirect.com/science/article/pii/S0167839616300978?via%3Dihub) 

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>


# Grants
- **2023.01 - 2025.12** Excellent Young Scientists Fund Program (Overseas) of Shandong Province (No.2023HWYQ-034), PI
- **2024.01 - 2026.12** Natural Science Foundation of Shandong Province (No. ZR2023QF077), PI
- **2024.01 - 2026.12** National Natural Science Foundation of China (No.62302269), PI
- **2024.11 - 2027.11** Embodied Intelligence Interaction Program from Leju Robotics, Participant
- **2024.12 - 2027.11**	National Key R&D Program of China (No. 2024YFB3309502), Participant

# Educations
- **09.2013 - 12.2018** PhD, Shandong University
- **09.2017 - 09.2018** Visiting PhD, Simon Fraser University
- **04.2017 - 05.2017** Visiting student, Tel Aviv University
- **03.2014 - 06.2014** Visiting student, University of Science and Technology of China
- **11.2013 - 01.2014** Research assistant, The University of Hong Kong


# Teaching
- **2023 - Now** Computer Graphics
- **2023 - Now** Computer Animation
- **2023 - Now** Graphical Design
- **2022 - 2023** Principles and Techniques of Compilation
