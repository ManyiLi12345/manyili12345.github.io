---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am now an associate researcher at the School of Software, Shandong University. Before my current position, I was a postdoc working with <a href="https://www.cs.sfu.ca/~haoz/">Prof. Hao (Richard) Zhang</a> in <a href="https://gruvi.cs.sfu.ca/">GrUVi Lab</a> at Simon Fraser University, Canada. I was also a intern research scientist in <a href="https://ips-ai.com/">Intelligent Project Solutions Inc.</a> working on Layout digitalization and editing projects. I got my Ph.D degree on December 2018 from Shandong University, supervised by <a href="http://irc.cs.sdu.edu.cn/~chtu/">Prof. Changhe Tu</a>. During my Ph.D career, I have visited and collaborated closely with 
                <a href="https://www.cs.sfu.ca/~haoz/">Prof. Hao (Richard) Zhang</a> (supported by 
                <a href="https://www.chinesescholarshipcouncil.com/">China Scholarship Council (CSC)</a>),
                <a href="http://www.math.tau.ac.il/~dcor/">Prof. Daniel Cohen-Or</a>,
                <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Prof. Wenping Wang</a>,
                <a href="http://staff.ustc.edu.cn/~chenfl/english.htm">Prof. Falai Chen</a>.

My research field lies in Computer Graphics and Computer Vision. More specifically, my research interest focuses on the understanding, generation, and interaction of 3D indoor scenes, including object reconstruction and segmentation, 3D shape analysis and generation, scene layout synthesis and editing, hand-object interaction generation, etc.

ææ›¼ç¥ï¼Œå±±ä¸œå¤§å­¦è½¯ä»¶å­¦é™¢äººæœºäº¤äº’ä¸è™šæ‹Ÿç°å®ä¸­å¿ƒå‰¯ç ”ç©¶å‘˜ï¼Œç¡•å£«ç”Ÿå¯¼å¸ˆã€‚2018å¹´12æœˆè·å¾—å±±ä¸œå¤§å­¦å·¥å­¦åšå£«å­¦ä½ï¼Œå¯¼å¸ˆä¸ºå± é•¿æ²³æ•™æˆã€‚è¯»åšæœŸé—´æ›¾åœ¨é¦™æ¸¯å¤§å­¦ã€ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ã€ç‰¹æ‹‰ç»´å¤«å¤§å­¦ã€è¥¿è’™å¼—é›·æ³½å¤§å­¦äº¤æµè®¿é—®ã€‚2019-2021å¹´åœ¨åŠ æ‹¿å¤§è¥¿è’™å¼—é›·æ³½å¤§å­¦GrUViå®éªŒå®¤ä»äº‹åšå£«åç ”ç©¶ï¼Œå¯¼å¸ˆä¸ºHao (Richard) Zhangæ•™æˆã€‚

ç ”ç©¶é¢†åŸŸä¸ºè®¡ç®—æœºå›¾å½¢å­¦ã€ä¸‰ç»´è§†è§‰ã€äººå·¥æ™ºèƒ½ç­‰ï¼Œä¸»è¦å…³æ³¨ä¸‰ç»´å®¤å†…åœºæ™¯çš„ç†è§£ã€ç”Ÿæˆä¸äº¤äº’ï¼ŒåŒ…æ‹¬ä¸‰ç»´ç‰©ä½“é‡å»ºä¸åˆ†å‰²ã€å½¢çŠ¶åˆ†æä¸ç”Ÿæˆã€åœºæ™¯å¸ƒå±€ç”Ÿæˆä¸ç¼–è¾‘ã€æ‰‹-ç‰©äº¤äº’ç”Ÿæˆç­‰ã€‚æ›¾åœ¨ACM Transactions on Graphics (TOG)ã€Siggraphã€CVPRã€ICCVã€TVCGç­‰é¡¶çº§å›½é™…ä¼šè®®å’ŒæœŸåˆŠä¸Šå‘è¡¨è®ºæ–‡åä½™ç¯‡ã€‚æ‹…ä»»ä¸­å›½è®¡ç®—æœºå­¦ä¼šè®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦ä¸“å§”ä¼šå§”å‘˜ã€ä¸­å›½å·¥ä¸šä¸åº”ç”¨æ•°å­¦å­¦ä¼šå‡ ä½•è®¾è®¡ä¸è®¡ç®—ä¸“å§”ä¼šç§˜ä¹¦å¤„å§”å‘˜ç­‰ã€‚ ç›®å‰ä¸»æŒå›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é’å¹´é¡¹ç›®ã€å±±ä¸œçœä¼˜ç§€é’å¹´ç§‘å­¦åŸºé‡‘é¡¹ç›®ï¼ˆæµ·å¤–ï¼‰ã€å±±ä¸œå¤§å­¦æœªæ¥è®¡åˆ’ç­‰é¡¹ç›®ã€‚è·å¾—2024å¹´CCFç§‘æŠ€æˆæœè‡ªç„¶ç§‘å­¦ä¸€ç­‰å¥–ã€2022å¹´ACMæµå—åˆ†ä¼šæ–°æ˜Ÿå¥–ã€‚


# ğŸ”¥ News
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ“ Publications 

<!-- CGF 2025  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CGF 2025</div><img src='images/paper_imgs/2025_diffusion_model_for_point_clouds.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model**

Zhaonan Wang , **Manyi Li\***, Shiqing Xin , Changhe Tu

**Paper** \| **Project**

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVMJ 2025  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVMJ 2025</div><img src='images/paper_imgs/2025_object_aware_transfer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Object-Aware Appearance Transfer for Interior Design**

Ruisi Ye, **Manyi Li\***, Xifeng Gao, Changhe Tu

**Paper** \| **Project**

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVPR 2025  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/paper_imgs/2025_freescene.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts**

Tongyuan Baiï¼Œ Wangyuanfan Baiï¼Œ Dong Chenï¼Œ Tieru Wu, **Manyi Li**ï¼Œ Rui Ma

**Paper** \| **Project**

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- AAAI 2025  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images/paper_imgs/2025_hierarchical_scenes.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Hierarchically-Structured Open-Vocabulary Indoor Scene Synthesis with Pre-trained Large Language Model**

Weilin Sun, Xinran Li, **Manyi Li**, 
<a href="https://kevinkaixu.net/">Kai Xu</a>, Xiangxu Meng, 
<a href="https://ercdm.sdu.edu.cn/info/1013/1523.htm">Lei Meng</a>

[**Paper**](https://arxiv.org/abs/2502.10675) \| [**Project**](https://github.com/SunWeiLin-Lynne/Hierarchically-Structured-Open-Vocabulary-Indoor-Scene-Synthesis/tree/main)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- Siggraph 2024  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Siggraph 2024</div><img src='images/paper_imgs/2024_dreamfont.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**DreamFont3D: Personalized Text-to-3D Artistic Font Generation**

Xiang Li, 
<a href="https://ercdm.sdu.edu.cn/info/1013/1523.htm">Lei Meng</a>, 
Lei Wu, **Manyi Li**, Xiangxu Meng

[**Paper**] \| [**Project**](https://moonlight03.github.io/DreamFont3D/)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CGF 2024  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CGF 2024</div><img src='images/paper_imgs/2024_scene_survey.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Advances in Data-Driven Analysis and Synthesis of 3D Indoor Scenes**

<a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>,
		      Supriya Gadi Patil,
		      **Manyi Li\***,
		      <a href="https://techmatt.github.io/">Matthew Fisher</a>,
		      <a href="http://msavva.github.io/">Manolis Savva</a>,
		      <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,

[**Paper**](https://arxiv.org/pdf/2304.03188.pdf) 

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- Siggraph Asia 2023  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Siggraph Asia 2024</div><img src='images/paper_imgs/2023_uvpack.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Learning Based 2D Irregular Shape Packing**

<a href="https://zeshiyang.github.io/">Zeshi Yang</a>,
		      <a>Zherong Pan</a>,
		      **Manyi Li**,
		      <a href="https://kuiwuchn.github.io/">Kui Wu</a>,
		      <a href="https://gaoxifeng.github.io/">Xifeng Gao</a>,

[**Paper**](https://arxiv.org/pdf/2309.10329.pdf) 

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- ICCV 2023  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/paper_imgs/2023_afford_pose.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**AffordPose: A Large-scale Dataset of Hand-Object Interactions with Affordance-driven Hand Pose**

Juntao Jian, Xiuping Liu, **Manyi Li\***, 
<a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>, 
<a href="https://jianliu2006.github.io/ ">Jian Liu*</a>

[**Paper**](https://arxiv.org/pdf/2309.08942.pdf) \| [**Project**](https://github.com/GentlesJan/AffordPose)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVMJ 2023  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVMJ 2023</div><img src='images/paper_imgs/2023_dancepro.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Let's all dance: Enhancing amateur dance motions**

Qiu Zhou#, **Manyi Li\#**, 
<a href="https://qiongzn.github.io/">Qiong Zeng</a>,
		      <a href="http://www.andreasaristidou.com/">Andreas Aristidou</a>, Xiaojing Zhang, Lin Chen, 
          <a href="http://irc.cs.sdu.edu.cn/~chtu/index.html">Changhe Tu</a>

[**Paper**](http://irc.cs.sdu.edu.cn/dancepro/dance_cvm.pdf) \| [**Project**](http://irc.cs.sdu.edu.cn/dancepro/index.html)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- TVCG 2023  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TVCG 2023</div><img src='images/paper_imgs/2023_laplacian2mesh.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Laplacian2mesh: Laplacian-based mesh understanding**

Qiujie Dong, 
<a href="https://bearprin.com/">Zixiong Wang</a>, 
**Manyi Li**, 
Junjie Gao, Shuang-Min Chen, Zhenyu Shu, 
<a href="https://irc.cs.sdu.edu.cn/~shiqing/index.html">Shiqing Xin</a>,
		      <a href="http://irc.cs.sdu.edu.cn/~chtu/index.html">Changhe Tu</a>,
		      <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a>

[**Paper**](https://arxiv.org/pdf/2202.00307.pdf) \| [**Project**](https://github.com/QiujieDong/Laplacian2Mesh)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVPR 2022  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/paper_imgs/2022_RIMNet_long.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Rim-net: Recursive implicit fields for unsupervised learning of hierarchical shape structures**

<a href="https://chengjieniu.github.io/">Chengjie Niu</a>,
              **Manyi Li**,
              <a href="https://kevinkaixu.net/">Kai Xu</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,

[**Paper**](https://arxiv.org/abs/2201.12763) \| [**Project**](https://github.com/chengjieniu/RIM-Net)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVPR 2022  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/paper_imgs/2022_capri_net.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly**

<a href="https://fenggenyu.github.io/">Fenggen Yu</a>,
              <a href="https://czq142857.github.io/">Zhiqin Chen</a>,
              **Manyi Li**,
			  <a>Aditya Sanghi</a>,
			  <a>Hooman Shayani</a>,
              <a href="https://sites.google.com/site/alimahdaviamiri/home">Ali Mahdavi-Amiri</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>

[**Paper**](https://arxiv.org/abs/2104.05652) \| [**Project**](https://fenggenyu.github.io/capri.html)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVPR 2021  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2021</div><img src='images/paper_imgs/2021_d2im_image.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**D^2IM-Net: Learning Detail Disentangled Implicit Fields from Single Images**

**Manyi Li**Â andÂ <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>

[**Paper**](https://arxiv.org/abs/2012.06650) \| [**Project**](https://github.com/ManyiLi12345/D2IM-Net)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CVPR 2021  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2021</div><img src='images/paper_imgs/2021_LayoutGMN_image.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**LayoutGMN: Neural Graph Matching for Structural Layout Similarity**

<a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>, 
**Manyi Li**,
              <a href="https://research.adobe.com/person/matt-fisher/">Matthew Fisher</a>,
              <a href="http://msavva.github.io/">Manolis Savva</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>

[**Paper**](https://arxiv.org/abs/2012.06547) \| [**Project**](https://github.com/akshaygadipatil/LayoutGMN-pytorch)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- TOG 2019  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TOG 2019</div><img src='images/paper_imgs/2019_grains_image.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**GRAINS: Generative Recursive Autoencoders for INdoor Scenes**

**Manyi Li**,
              <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil</a>,
              <a href="https://kevinkaixu.net/">Kai Xu</a>,
              <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>,
              Owais Khan,
              <a href="https://faculty.idc.ac.il/arik/site/index.asp">Ariel Shamir</a>,
              <a href="http://irc.cs.sdu.edu.cn/~chtu/">Changhe Tu</a>,
              <a href="http://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>,
              <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,

[**Paper**](https://arxiv.org/abs/1807.09193) \| [**Project**](./Publication/2018/GRAINS/index.html)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- Siggraph Asia 2018  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Siggraph Asia 2018</div><img src='images/paper_imgs/2018_T2S_image.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Language-driven synthesis of 3D scenes from scene databases**

<a href="https://ruim-jlu.github.io/">Rui Ma#</a>,
              <a href="http://www.sfu.ca/~agadipat/">Akshay Gadi Patil#</a>,
              <a href="https://techmatt.github.io/">Matthew Fisher</a>,
              **Manyi Li**,
              <a href="https://storage.googleapis.com/pirk.io/index.html">SÃ¶ren Pirk</a>,
              <a href="http://sonhua.github.io/">Binh-Son Hua</a>,
              <a href="http://www.saikit.org/">Sai-Kit Yeung</a>,
              <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>,
              <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,

[**Paper**](./Publication/2018/T2S/t2s_final.pdf) \| [**Project**](./Publication/2018/T2S/index.html)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- GMOD 2018  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GMOD 2018</div><img src='images/paper_imgs/2018_ShapeDist_image.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Class-sensitive shape dissimilarity metric**

**Manyi Li**, <a href="https://www.cs.tau.ac.il/~noafish/">Noa Fish</a>,
              Lili Cheng,
              <a href="http://irc.cs.sdu.edu.cn/~chtu/">Changhe Tu</a>,
              <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
              <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a>,  
              <a href="http://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>

[**Paper**](https://www.sciencedirect.com/science/article/pii/S1524070318300328?via%3Dihub)

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>

<!-- CAGD 2016  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GMOD 2018</div><img src='images/paper_imgs/2016_SparseRBF_image.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Sparse RBF surface representations**

**Manyi Li**,Â <a href="http://staff.ustc.edu.cn/~chenfl/english.htm">Falai Chen</a>,
              <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Wenping Wang</a>,
              <a href="http://irc.cs.sdu.edu.cn/~chtu/">Changhe Tu</a>

[**Paper**](https://www.sciencedirect.com/science/article/pii/S0167839616300978?via%3Dihub) 

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.-->
</div>
</div>


# Grants
- **2023.01 - 2025.12** Excellent Young Scientists Fund Program (Overseas) of Shandong Province (No.2023HWYQ-034), PI
- **2024.01 - 2026.12** Natural Science Foundation of Shandong Province (No. ZR2023QF077), PI
- **2024.01 - 2026.12** National Natural Science Foundation of China (No.62302269), PI
- **2024.11 - 2027.11** Embodied Intelligence Interaction Program from Leju Robotics, Participant
- **2024.12 - 2027.11**	National Key R&D Program of China (No. 2024YFB3309502), Participant

# Educations
- **09.2013 - 12.2018** PhD, Shandong University
- **09.2017 - 09.2018** Visiting PhD, Simon Fraser University
- **04.2017 - 05.2017** Visiting student, Tel Aviv University
- **03.2014 - 06.2014** Visiting student, University of Science and Technology of China
- **11.2013 - 01.2014** Research assistant, The University of Hong Kong


# Teaching
- **2023 - Now** Computer Graphics
- **2023 - Now** Computer Animation
- **2023 - Now** Graphical Design
- **2022 - 2023** Principles and Techniques of Compilation
